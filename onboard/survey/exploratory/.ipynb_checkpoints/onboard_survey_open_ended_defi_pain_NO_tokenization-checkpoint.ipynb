{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw/Onboard_Survey.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip first seven columns df.iloc[:, 0:7].head()\n",
    "\n",
    "# selecting only open-ended responses \n",
    "df.iloc[:, 6:14].head()\n",
    "\n",
    "open_ended = df.iloc[:, 6:14]\n",
    "\n",
    "open_ended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rename columns to better manage columns\n",
    "open_ended.columns = ['walletwhat_walletwhy', 'wallet_pain', 'defi_when', 'defiwhat_defiwhy', 'defi_pain', 'defi_outcome', 'defi_interest', 'defi_endgame']\n",
    "\n",
    "\n",
    "open_ended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKIP stemming & tokenzation to see if yields more interpretable results ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is Part 2 of Onboard Survey Exploratory Analysis\n",
    "\n",
    "# For Part 1 see onboard_survey_open_ended.ipynb\n",
    "# For Part 1 https://forum.bankless.community/t/onboard-survey-exploratory-analysis/1048\n",
    "\n",
    "# Part 2 Open-Ended questions to address include:\n",
    "\n",
    "# What has been painful about using DeFi apps or what has or is an obstacle in your way to using a DeFi app? [column: defi_pain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling\n",
    "\n",
    "# Preparing Text Data for NLP\n",
    "# Goal: Turn text data in to matrix (row = document, column = feature)\n",
    "\n",
    "# Steps: \n",
    "\n",
    "# forming a corpus of text\n",
    "# stemming and lemmatization --- SKIP\n",
    "# tokenization               --- SKIP\n",
    "# removing stop-words\n",
    "# finding words co-located together (N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stemming & Lemmatization to defi_pain\n",
    "\n",
    "# take entire column in open_ended df\n",
    "# split sentences (each row) into words\n",
    "# store in empty list\n",
    "\n",
    "defi_pain_list = []\n",
    "\n",
    "# 12 Rows Removed\n",
    "for row in open_ended['defi_pain']:\n",
    "    try:\n",
    "        defi_pain_list.append(row.split())\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "defi_pain_list  # this is a Nested list - list of list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning',\n",
       " 'how',\n",
       " 'to',\n",
       " 'navigate',\n",
       " 'web3',\n",
       " 'websites.',\n",
       " 'Gas',\n",
       " 'fees',\n",
       " 'are',\n",
       " 'fluctuating',\n",
       " 'each',\n",
       " 'second',\n",
       " 'Terrible',\n",
       " 'UIUX',\n",
       " 'and',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'understand',\n",
       " 'it',\n",
       " 'all.',\n",
       " 'Gas',\n",
       " 'fees',\n",
       " 'gas',\n",
       " 'fees',\n",
       " 'on',\n",
       " 'eth',\n",
       " 'main',\n",
       " 'net.',\n",
       " 'resolved',\n",
       " 'by',\n",
       " 'doing',\n",
       " 'most',\n",
       " 'things',\n",
       " 'on',\n",
       " 'polygon',\n",
       " 'Gas',\n",
       " 'fees',\n",
       " 'Lack',\n",
       " 'of',\n",
       " 'user',\n",
       " 'friendly',\n",
       " 'interfaces',\n",
       " 'or',\n",
       " 'documentation',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'detailed',\n",
       " 'enough',\n",
       " 'gas',\n",
       " 'costs',\n",
       " 'Ignoring',\n",
       " 'the',\n",
       " 'obvious',\n",
       " 'gas',\n",
       " 'fees',\n",
       " 'answer,',\n",
       " \"I'm\",\n",
       " 'bad',\n",
       " 'at',\n",
       " 'math,',\n",
       " 'and',\n",
       " 'doing',\n",
       " 'the',\n",
       " 'more',\n",
       " 'advanced',\n",
       " 'things',\n",
       " 'like',\n",
       " 'providing',\n",
       " 'liquidity',\n",
       " 'concern',\n",
       " 'me.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'strike',\n",
       " 'a',\n",
       " 'more',\n",
       " 'conservative',\n",
       " 'risk/reward',\n",
       " 'balance.',\n",
       " 'Gas',\n",
       " 'fees.',\n",
       " 'Also',\n",
       " 'not',\n",
       " 'understand',\n",
       " 'liquidation',\n",
       " 'or',\n",
       " 'IL.',\n",
       " 'Not',\n",
       " 'having',\n",
       " 'an',\n",
       " 'easy',\n",
       " 'way',\n",
       " 'to',\n",
       " 'play',\n",
       " 'with',\n",
       " 'defi',\n",
       " 'apps.',\n",
       " 'Best',\n",
       " 'thing',\n",
       " 'I',\n",
       " 'can',\n",
       " 'do',\n",
       " 'is',\n",
       " 'go',\n",
       " 'to',\n",
       " 'an',\n",
       " 'L2',\n",
       " 'network',\n",
       " 'and',\n",
       " 'play',\n",
       " 'around,',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'disadvantages',\n",
       " 'to',\n",
       " 'that',\n",
       " 'method.',\n",
       " 'Basically',\n",
       " 'just',\n",
       " 'having',\n",
       " 'a',\n",
       " 'sandbox',\n",
       " 'to',\n",
       " 'touch',\n",
       " 'and',\n",
       " 'feel',\n",
       " 'the',\n",
       " 'protocol.',\n",
       " 'Rug',\n",
       " 'pulls;',\n",
       " 'unclear',\n",
       " 'token',\n",
       " 'mechanics',\n",
       " 'Understanding',\n",
       " 'why',\n",
       " 'pairs',\n",
       " 'are',\n",
       " 'necessary',\n",
       " 'for',\n",
       " 'AMMs,',\n",
       " 'then',\n",
       " 'things',\n",
       " 'like',\n",
       " 'Impermanent',\n",
       " 'Loss',\n",
       " 'not',\n",
       " 'really',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'impact',\n",
       " 'of',\n",
       " 'IL,',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'executing',\n",
       " 'a',\n",
       " 'smart',\n",
       " 'contract,',\n",
       " 'keeping',\n",
       " 'track',\n",
       " 'of',\n",
       " 'P&L',\n",
       " 'Most',\n",
       " 'assume',\n",
       " 'that',\n",
       " 'you',\n",
       " 'already',\n",
       " 'know',\n",
       " 'what',\n",
       " \"you're\",\n",
       " 'doing.',\n",
       " 'Less',\n",
       " 'clear',\n",
       " 'for',\n",
       " 'noobs.',\n",
       " 'Lack',\n",
       " 'of',\n",
       " 'documentation',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'confirmation',\n",
       " 'or',\n",
       " 'having',\n",
       " 'to',\n",
       " 'do',\n",
       " 'different',\n",
       " 'steps',\n",
       " 'Managing',\n",
       " 'gas',\n",
       " 'is',\n",
       " 'really',\n",
       " 'something',\n",
       " 'I',\n",
       " 'wish',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'have',\n",
       " 'to',\n",
       " 'manage',\n",
       " 'as',\n",
       " 'an',\n",
       " 'Ethereum',\n",
       " 'user,',\n",
       " 'but',\n",
       " 'I',\n",
       " 'hear',\n",
       " 'its',\n",
       " 'use',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'much',\n",
       " 'nicer',\n",
       " 'with',\n",
       " 'EIP-1559',\n",
       " 'ðŸ™‚.',\n",
       " 'aside',\n",
       " 'from',\n",
       " 'gas',\n",
       " 'fees,',\n",
       " 'keeping',\n",
       " 'track',\n",
       " 'of',\n",
       " 'promised',\n",
       " 'yield',\n",
       " 'vs',\n",
       " 'actual',\n",
       " 'yield,',\n",
       " 'especially',\n",
       " 'when',\n",
       " 'yield',\n",
       " 'pays',\n",
       " 'out',\n",
       " 'in',\n",
       " 'multiple',\n",
       " 'non-ETH',\n",
       " 'token',\n",
       " 'is',\n",
       " 'difficult.',\n",
       " 'leaves',\n",
       " 'you',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'the',\n",
       " 'work',\n",
       " 'it',\n",
       " 'takes',\n",
       " 'to',\n",
       " 'vet',\n",
       " 'opportunities',\n",
       " 'and',\n",
       " 'stay',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'yield',\n",
       " 'is',\n",
       " 'paying',\n",
       " 'off',\n",
       " 'If',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'have',\n",
       " 'an',\n",
       " 'investing',\n",
       " 'background',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'jargon',\n",
       " 'creates',\n",
       " 'hesitancy,',\n",
       " 'uncertainty,',\n",
       " 'and',\n",
       " 'just',\n",
       " 'plain,',\n",
       " 'old,',\n",
       " '\"what',\n",
       " 'the',\n",
       " 'hell',\n",
       " 'am',\n",
       " 'I',\n",
       " 'doing?\"',\n",
       " 'and',\n",
       " 'crossing',\n",
       " 'fingers.',\n",
       " 'Fees,',\n",
       " 'forgetting',\n",
       " 'my',\n",
       " 'keys',\n",
       " 'too',\n",
       " 'expensive',\n",
       " 'confirming',\n",
       " 'correct',\n",
       " 'addresses',\n",
       " 'etc,',\n",
       " 'making',\n",
       " 'sure',\n",
       " 'i',\n",
       " 'wasnt',\n",
       " 'making',\n",
       " 'mistakes.',\n",
       " \"I've\",\n",
       " 'currently',\n",
       " 'staked',\n",
       " 'some',\n",
       " 'ETH',\n",
       " 'with',\n",
       " 'Lido',\n",
       " '-',\n",
       " 'and',\n",
       " 'I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'further',\n",
       " 'use',\n",
       " 'stETH',\n",
       " 'to',\n",
       " 'generate',\n",
       " 'yields,',\n",
       " 'but',\n",
       " 'not',\n",
       " 'quite',\n",
       " 'sure',\n",
       " 'how',\n",
       " 'to',\n",
       " 'do',\n",
       " 'that.',\n",
       " \"I'm\",\n",
       " 'reading',\n",
       " 'through',\n",
       " 'their',\n",
       " 'blog',\n",
       " 'post',\n",
       " 'on',\n",
       " 'Curve',\n",
       " '-',\n",
       " \"it's\",\n",
       " 'not',\n",
       " 'straight',\n",
       " 'forward.',\n",
       " 'High',\n",
       " 'fees,',\n",
       " 'uncertainty',\n",
       " 'around',\n",
       " 'security',\n",
       " 'Wallet',\n",
       " 'compatibility',\n",
       " 'is',\n",
       " 'always',\n",
       " 'an',\n",
       " 'obstacle.',\n",
       " 'Not',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'use',\n",
       " 'Alchemix',\n",
       " 'with',\n",
       " 'my',\n",
       " 'usual',\n",
       " 'wallets',\n",
       " 'was',\n",
       " 'painful.',\n",
       " 'But',\n",
       " \"that's\",\n",
       " 'not',\n",
       " 'only',\n",
       " 'concerning',\n",
       " 'Defi,',\n",
       " 'for',\n",
       " 'example',\n",
       " 'claiming',\n",
       " 'the',\n",
       " 'BANK',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'to',\n",
       " 'be',\n",
       " 'impossible',\n",
       " 'with',\n",
       " 'Argent,',\n",
       " 'same',\n",
       " 'with',\n",
       " 'obtaining',\n",
       " 'the',\n",
       " 'Discord',\n",
       " 'access.',\n",
       " 'Other',\n",
       " 'than',\n",
       " 'that,',\n",
       " 'my',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'technical',\n",
       " 'expertise',\n",
       " '(like',\n",
       " 'reading',\n",
       " 'code)',\n",
       " 'always',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'anxious',\n",
       " 'about',\n",
       " 'using',\n",
       " 'a',\n",
       " 'protocol,',\n",
       " 'not',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'push',\n",
       " 'me',\n",
       " 'to',\n",
       " 'start',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'it,',\n",
       " 'but',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'make',\n",
       " 'me',\n",
       " 'wait',\n",
       " 'and',\n",
       " 'miss',\n",
       " 'opportunities.',\n",
       " 'Gas',\n",
       " 'fees',\n",
       " 'Bad',\n",
       " 'UI',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'LP',\n",
       " 'and',\n",
       " 'perform',\n",
       " 'other',\n",
       " 'Defi',\n",
       " 'tasks.',\n",
       " 'Usually',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'research',\n",
       " 'before',\n",
       " 'I',\n",
       " 'can',\n",
       " 'figure',\n",
       " 'out',\n",
       " 'the',\n",
       " 'processes',\n",
       " 'User',\n",
       " 'Experience',\n",
       " '-',\n",
       " 'UI',\n",
       " 'Gas',\n",
       " 'prices',\n",
       " 'Understanding',\n",
       " 'what',\n",
       " 'it',\n",
       " 'does,',\n",
       " 'why',\n",
       " 'I',\n",
       " 'need',\n",
       " 'it,',\n",
       " 'or',\n",
       " 'how',\n",
       " 'I',\n",
       " 'could',\n",
       " 'benefit',\n",
       " 'by',\n",
       " 'using',\n",
       " 'it',\n",
       " 'Mystery',\n",
       " 'around',\n",
       " 'security',\n",
       " '(\"Am',\n",
       " 'i',\n",
       " 'doing',\n",
       " 'it',\n",
       " 'right?\")',\n",
       " 'Depositing',\n",
       " 'funds',\n",
       " 'into',\n",
       " 'Defi',\n",
       " 'apps',\n",
       " 'High',\n",
       " 'fees',\n",
       " 'to',\n",
       " 'transfer',\n",
       " 'funds',\n",
       " 'along',\n",
       " 'the',\n",
       " 'network',\n",
       " 'The',\n",
       " 'fees',\n",
       " 'is',\n",
       " 'the',\n",
       " 'obvious',\n",
       " 'answer.',\n",
       " 'But',\n",
       " \"it's\",\n",
       " 'also',\n",
       " 'the',\n",
       " 'learning',\n",
       " 'curve.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'always',\n",
       " 'be',\n",
       " 'like',\n",
       " 'that',\n",
       " 'though.',\n",
       " \"It's\",\n",
       " 'the',\n",
       " 'same',\n",
       " 'in',\n",
       " 'traditional',\n",
       " 'finance',\n",
       " 'like',\n",
       " 'with',\n",
       " 'derivates',\n",
       " 'etc.',\n",
       " 'If',\n",
       " 'something',\n",
       " 'is',\n",
       " 'complicated',\n",
       " 'then',\n",
       " \"it's\",\n",
       " 'just',\n",
       " 'complicated,',\n",
       " 'but',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'easier',\n",
       " 'the',\n",
       " 'more',\n",
       " 'you',\n",
       " 'use',\n",
       " 'it.',\n",
       " 'Keeping',\n",
       " 'track',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'different',\n",
       " 'dapps',\n",
       " 'without',\n",
       " 'a',\n",
       " 'great',\n",
       " 'centralized',\n",
       " 'tool',\n",
       " 'to',\n",
       " 'monitor',\n",
       " 'them',\n",
       " 'all.',\n",
       " 'Great',\n",
       " 'UX',\n",
       " 'for',\n",
       " 'crypto-natives,',\n",
       " 'not',\n",
       " 'so',\n",
       " 'much',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'us',\n",
       " '(I',\n",
       " 'consider',\n",
       " 'myself',\n",
       " 'a',\n",
       " 'crypto',\n",
       " 'native',\n",
       " 'now,',\n",
       " 'though)',\n",
       " 'Most',\n",
       " 'defi',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'painful',\n",
       " 'because',\n",
       " \"they're\",\n",
       " 'not',\n",
       " '\"easy\"',\n",
       " 'to',\n",
       " 'use.',\n",
       " 'Many',\n",
       " 'steps',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'involved',\n",
       " 'and',\n",
       " \"it's\",\n",
       " 'complicated.',\n",
       " \"It's\",\n",
       " 'especially',\n",
       " 'complicated',\n",
       " 'when',\n",
       " 'you',\n",
       " 'barely',\n",
       " 'understand',\n",
       " 'how',\n",
       " 'it',\n",
       " 'works',\n",
       " 'and',\n",
       " 'so',\n",
       " 'that',\n",
       " 'can',\n",
       " 'make',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'process',\n",
       " 'overwhelming.',\n",
       " 'Gas!',\n",
       " 'to',\n",
       " 'Connect',\n",
       " 'my',\n",
       " 'wallet',\n",
       " 'I',\n",
       " 'have',\n",
       " 'to',\n",
       " 'reload,',\n",
       " 'transaction',\n",
       " 'appear',\n",
       " 'after',\n",
       " 'some',\n",
       " 'time,',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'prices',\n",
       " 'or',\n",
       " 'apy',\n",
       " 'does',\n",
       " 'not',\n",
       " 'load',\n",
       " 'Complexity,',\n",
       " 'uncertainty',\n",
       " 'Knowing',\n",
       " 'what',\n",
       " 'to',\n",
       " 'trust',\n",
       " 'and',\n",
       " 'what',\n",
       " 'not',\n",
       " 'to',\n",
       " 'trust',\n",
       " 'High',\n",
       " 'gas',\n",
       " 'costs',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'what',\n",
       " \"I'm\",\n",
       " 'looking',\n",
       " 'for,',\n",
       " 'what',\n",
       " 'to',\n",
       " 'trust,',\n",
       " 'what',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'from,',\n",
       " 'etc',\n",
       " 'Confirming',\n",
       " 'if',\n",
       " 'Iâ€™m',\n",
       " 'in',\n",
       " 'the',\n",
       " 'real',\n",
       " 'site',\n",
       " 'or',\n",
       " 'a',\n",
       " 'phish.',\n",
       " 'Would',\n",
       " 'be',\n",
       " 'nice',\n",
       " 'if',\n",
       " 'defi',\n",
       " 'could',\n",
       " 'be',\n",
       " 'bookmarked/signed',\n",
       " 'as',\n",
       " 'authentic',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bookmark',\n",
       " 'I',\n",
       " 'like',\n",
       " 'them',\n",
       " 'Confusing!',\n",
       " 'I',\n",
       " 'may',\n",
       " 'have',\n",
       " 'lost',\n",
       " 'some',\n",
       " 'funds',\n",
       " 'because',\n",
       " 'I',\n",
       " 'did',\n",
       " 'some',\n",
       " 'transactions',\n",
       " 'out',\n",
       " 'of',\n",
       " 'order.',\n",
       " 'Gas',\n",
       " 'fees',\n",
       " 'There',\n",
       " 'was',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'a',\n",
       " 'learning',\n",
       " 'curve',\n",
       " 'to',\n",
       " 'finding',\n",
       " 'out',\n",
       " 'how',\n",
       " 'and',\n",
       " 'where',\n",
       " 'to',\n",
       " 'stake',\n",
       " 'then',\n",
       " 'where',\n",
       " 'to',\n",
       " 'put',\n",
       " 'that',\n",
       " 'tokenized',\n",
       " 'LP',\n",
       " 'position.',\n",
       " 'Also,',\n",
       " 'transaction',\n",
       " 'fees',\n",
       " 'on',\n",
       " 'main',\n",
       " 'net',\n",
       " 'getting',\n",
       " 'actual',\n",
       " \"APY's\",\n",
       " 'and',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'profitability',\n",
       " 'of',\n",
       " 'my',\n",
       " 'positions',\n",
       " 'Learning',\n",
       " 'about',\n",
       " 'the',\n",
       " 'risks,',\n",
       " 'getting',\n",
       " 'rugged',\n",
       " '(on',\n",
       " 'bsc)',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'slippage,',\n",
       " 'rounding',\n",
       " 'etcâ€¦.',\n",
       " 'Now',\n",
       " 'am',\n",
       " 'bullish',\n",
       " 'af',\n",
       " 'about',\n",
       " 'all',\n",
       " 'things',\n",
       " 'defi',\n",
       " 'Gas',\n",
       " 'fees',\n",
       " 'to',\n",
       " 'â€˜approveâ€™',\n",
       " 'tokens',\n",
       " 'Gas',\n",
       " 'fees',\n",
       " 'Not',\n",
       " 'calculating',\n",
       " 'the',\n",
       " 'gas',\n",
       " 'fees',\n",
       " 'before',\n",
       " 'withdrawing',\n",
       " 'my',\n",
       " 'funds.',\n",
       " 'I',\n",
       " 'ended',\n",
       " 'up',\n",
       " 'breaking',\n",
       " 'even',\n",
       " 'because',\n",
       " 'of',\n",
       " 'this.',\n",
       " 'Gas',\n",
       " 'fees',\n",
       " 'and',\n",
       " 'impermanent',\n",
       " 'loss',\n",
       " 'Number',\n",
       " 'of',\n",
       " 'required',\n",
       " 'steps',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'Lately,',\n",
       " 'gas',\n",
       " 'prices.',\n",
       " 'Gas',\n",
       " 'price',\n",
       " 'There',\n",
       " 'are',\n",
       " 'so',\n",
       " 'many!',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'decide',\n",
       " 'which',\n",
       " 'to',\n",
       " 'participate',\n",
       " 'in.',\n",
       " 'The',\n",
       " 'gas',\n",
       " 'fees..',\n",
       " 'The',\n",
       " 'language',\n",
       " 'was',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'barrier',\n",
       " 'to',\n",
       " 'entry.',\n",
       " 'Liquidity,',\n",
       " 'vaults,',\n",
       " 'gas,',\n",
       " 'pairs,',\n",
       " 'staking,',\n",
       " 'etc.',\n",
       " 'Once',\n",
       " 'I',\n",
       " 'learned',\n",
       " 'the',\n",
       " 'language',\n",
       " 'then',\n",
       " 'I',\n",
       " 'fell',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'the',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'it',\n",
       " 'all.',\n",
       " 'Separate',\n",
       " 'price',\n",
       " 'feeds',\n",
       " 'and',\n",
       " 'token',\n",
       " 'support',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'get',\n",
       " 'my',\n",
       " 'metamask',\n",
       " 'and',\n",
       " 'sollet',\n",
       " 'set',\n",
       " 'up',\n",
       " 'properly',\n",
       " 'GAS',\n",
       " 'Fees',\n",
       " 'ethereum',\n",
       " 'gas',\n",
       " 'fees.',\n",
       " 'Fees.',\n",
       " 'One',\n",
       " 'time',\n",
       " 'I',\n",
       " 'paid',\n",
       " '~100',\n",
       " '$',\n",
       " 'fee',\n",
       " 'for',\n",
       " 'transaction',\n",
       " 'that',\n",
       " 'has',\n",
       " 'had',\n",
       " 'their',\n",
       " 'own',\n",
       " 'value',\n",
       " 'much',\n",
       " 'less',\n",
       " 'than',\n",
       " 'that',\n",
       " 'fee',\n",
       " 'value.',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'instructions',\n",
       " 'and',\n",
       " 'uncertainty',\n",
       " 'of',\n",
       " 'what',\n",
       " 'to',\n",
       " 'do',\n",
       " 'on',\n",
       " 'farm',\n",
       " 'pages',\n",
       " 'Just',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'know',\n",
       " 'what',\n",
       " 'to',\n",
       " 'push',\n",
       " 'when',\n",
       " 'to',\n",
       " 'accomplish',\n",
       " 'the',\n",
       " 'task.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'claim',\n",
       " 'my',\n",
       " '2021',\n",
       " 'POAP',\n",
       " 'Bankless',\n",
       " 'badge',\n",
       " 'without',\n",
       " 'assistance',\n",
       " 'using',\n",
       " 'my',\n",
       " 'IPad',\n",
       " 'by',\n",
       " 'Above',\n",
       " 'Average',\n",
       " 'Joe',\n",
       " 'spent',\n",
       " 'hours',\n",
       " 'helping',\n",
       " 'me',\n",
       " 'claim',\n",
       " 'my',\n",
       " '37,422+',\n",
       " 'Bank',\n",
       " 'and',\n",
       " 'getting',\n",
       " 'me',\n",
       " 'sorted',\n",
       " 'by',\n",
       " 'shifting',\n",
       " 'to',\n",
       " 'a',\n",
       " 'laptop',\n",
       " 'better',\n",
       " 'server',\n",
       " 'zoom',\n",
       " 'etc',\n",
       " '3',\n",
       " 'different',\n",
       " 'sessions.',\n",
       " 'Also',\n",
       " 'AA',\n",
       " 'Joe',\n",
       " 'had',\n",
       " 'to',\n",
       " 'assist',\n",
       " 'with',\n",
       " '2nd',\n",
       " 'retroactive',\n",
       " 'Bank',\n",
       " 'drop',\n",
       " 'and',\n",
       " 'I',\n",
       " 'still',\n",
       " 'have',\n",
       " 'not',\n",
       " 'been',\n",
       " 'able',\n",
       " 'to',\n",
       " 'figure',\n",
       " 'out',\n",
       " 'claiming',\n",
       " 'POAP',\n",
       " 'badges',\n",
       " 'for',\n",
       " 'attending',\n",
       " 'community',\n",
       " 'calls.',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last step need to FLATTEN a list of lists into one list/vector of words - \"Bag of Words\"\n",
    "# Bag of word, a list cleaned of punctuation, stemmed, now a vector of individual words\n",
    "\n",
    "defi_pain_list_flat = [item for sublist in defi_pain_list for item in sublist]\n",
    "\n",
    "defi_pain_list_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF: Weighting terms based on frequency\n",
    "\n",
    "# re-weights words to emphasize words that are unique to a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n",
      "lie\n",
      "systemat\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# Example of how a Stemmer works - - Only used for Stopwords in THIS Notebook\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem('lying'))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Function - - Only used for Stopwords in THIS Notebook\n",
    "\n",
    "def tokenize(text):\n",
    "    translator=str.maketrans(string.punctuation, ' '*len(string.punctuation)) # translator replace punct w empty space\n",
    "    return [stemmer.stem(i) for i in text.translate(translator).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'our',\n",
       " 'ourselv',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'your',\n",
       " 'your',\n",
       " 'yourself',\n",
       " 'yourselv',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'she',\n",
       " 'her',\n",
       " 'her',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'it',\n",
       " 'it',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'their',\n",
       " 'themselv',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'be',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'have',\n",
       " 'do',\n",
       " 'doe',\n",
       " 'did',\n",
       " 'do',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'becaus',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'dure',\n",
       " 'befor',\n",
       " 'after',\n",
       " 'abov',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'onc',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whi',\n",
       " 'how',\n",
       " 'all',\n",
       " 'ani',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'onli',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'veri',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'don',\n",
       " 'should',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'wouldn',\n",
       " 'invent',\n",
       " 'produc',\n",
       " 'method',\n",
       " 'use',\n",
       " 'first',\n",
       " 'second']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Stopwords\n",
    "stop = stopwords.words('english') + ['invent', 'produce', 'method', 'use', 'first', 'second']\n",
    "full_stopwords = [tokenize(s)[0] for s in stop]\n",
    "\n",
    "full_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 gwei, select, simplic, action, riski, howev, huge, 5, earli, trial\n",
      "1 certain, platform, rate, dead, v3, found, featur, sketchi, lps, ledger\n",
      "2 faith, rare, emiss, leap, ground, teach, near, thank, lower, crazi\n",
      "3 exposur, area, volatil, function, fair, rural, exact, uni, simpl, due\n",
      "4 almost, prohibit, gas transact, stablecoin, sinc, faint, ok, cefi, incurr, known\n"
     ]
    }
   ],
   "source": [
    "tf_defi_pain_vectorizer = CountVectorizer(analyzer= 'word',  # unit of features are single words rather than phrases\n",
    "                               tokenizer=tokenize, # function to create tokens\n",
    "                               ngram_range=(0,2),   # Allow for bigrams\n",
    "                               strip_accents='unicode',\n",
    "                               stop_words=full_stopwords,  # see above Example Stopwords, other examples did NOT hv stop_words\n",
    "                               min_df = 0.0,\n",
    "                               max_df = 1)   # got an error to lower min_df and raise max_df\n",
    "\n",
    "# Creating bag of words \n",
    "tf_defi_pain_bag_of_words = tf_defi_pain_vectorizer.fit_transform(defi_pain_list_flat) # IMPORTANT transform our UN-Tokenized (no stemming) corpus into a bag of words\n",
    "tf_defi_pain_features = tf_defi_pain_vectorizer.get_feature_names()\n",
    "\n",
    "# Use TfidfTransformer (see library import) to re-weight bag of words\n",
    "tf_defi_pain_transformer = TfidfTransformer(norm = None, smooth_idf = True, sublinear_tf = True)\n",
    "tf_defi_pain_tfidf = tf_defi_pain_transformer.fit_transform(tf_defi_pain_bag_of_words)\n",
    "\n",
    "# Fitting LDA Model\n",
    "tf_defi_pain_lda = LatentDirichletAllocation(n_components = 5, learning_method='online')  # NOTE: n_components = 5\n",
    "tf_defi_pain_doctopic = tf_defi_pain_lda.fit_transform(tf_defi_pain_tfidf)\n",
    "\n",
    "# Displaying the top keywords in each topic\n",
    "tf_defi_pain_keywords_list = []\n",
    "\n",
    "\n",
    "for i, topic in enumerate(tf_defi_pain_lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:10]     # NOTE: 10 instead of 5\n",
    "    tf_defi_pain_keywords = ', '.join(tf_defi_pain_features[i] for i in word_idx)\n",
    "    tf_defi_pain_keywords_list.append(tf_defi_pain_keywords)\n",
    "    print(i, tf_defi_pain_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gwei, select, simplic, action, riski, howev, huge, 5, earli, trial',\n",
       " 'certain, platform, rate, dead, v3, found, featur, sketchi, lps, ledger',\n",
       " 'faith, rare, emiss, leap, ground, teach, near, thank, lower, crazi',\n",
       " 'exposur, area, volatil, function, fair, rural, exact, uni, simpl, due',\n",
       " 'almost, prohibit, gas transact, stablecoin, sinc, faint, ok, cefi, incurr, known']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_defi_pain_keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       ...,\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.02200125, 0.02200123, 0.02200123, 0.91199506, 0.02200124]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_defi_pain_doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gwei, select, simplic, action, riski, howev, huge, 5, earli, trial</th>\n",
       "      <th>certain, platform, rate, dead, v3, found, featur, sketchi, lps, ledger</th>\n",
       "      <th>faith, rare, emiss, leap, ground, teach, near, thank, lower, crazi</th>\n",
       "      <th>exposur, area, volatil, function, fair, rural, exact, uni, simpl, due</th>\n",
       "      <th>almost, prohibit, gas transact, stablecoin, sinc, faint, ok, cefi, incurr, known</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022002</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.911994</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.911994</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gwei, select, simplic, action, riski, howev, huge, 5, earli, trial  \\\n",
       "0                                           0.200000                    \n",
       "1                                           0.200000                    \n",
       "2                                           0.200000                    \n",
       "3                                           0.022002                    \n",
       "4                                           0.911994                    \n",
       "\n",
       "   certain, platform, rate, dead, v3, found, featur, sketchi, lps, ledger  \\\n",
       "0                                           0.200000                        \n",
       "1                                           0.200000                        \n",
       "2                                           0.200000                        \n",
       "3                                           0.022001                        \n",
       "4                                           0.022001                        \n",
       "\n",
       "   faith, rare, emiss, leap, ground, teach, near, thank, lower, crazi  \\\n",
       "0                                           0.200000                    \n",
       "1                                           0.200000                    \n",
       "2                                           0.200000                    \n",
       "3                                           0.022001                    \n",
       "4                                           0.022001                    \n",
       "\n",
       "   exposur, area, volatil, function, fair, rural, exact, uni, simpl, due  \\\n",
       "0                                           0.200000                       \n",
       "1                                           0.200000                       \n",
       "2                                           0.200000                       \n",
       "3                                           0.911994                       \n",
       "4                                           0.022001                       \n",
       "\n",
       "   almost, prohibit, gas transact, stablecoin, sinc, faint, ok, cefi, incurr, known  \n",
       "0                                           0.200000                                 \n",
       "1                                           0.200000                                 \n",
       "2                                           0.200000                                 \n",
       "3                                           0.022001                                 \n",
       "4                                           0.022001                                 "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defi_pain_df = pd.DataFrame(tf_defi_pain_doctopic, columns = tf_defi_pain_keywords_list)\n",
    "\n",
    "defi_pain_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
