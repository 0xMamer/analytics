{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "import string\n",
    "\n",
    "from pprint import pprint  # PRETTY PRINT long dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw/Onboard_Survey.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip first seven columns df.iloc[:, 0:7].head()\n",
    "\n",
    "# selecting only open-ended responses \n",
    "df.iloc[:, 6:14].head()\n",
    "\n",
    "open_ended = df.iloc[:, 6:14]\n",
    "\n",
    "open_ended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rename columns to better manage columns\n",
    "open_ended.columns = ['walletwhat_walletwhy', 'wallet_pain', 'defi_when', 'defiwhat_defiwhy', 'defi_pain', 'defi_outcome', 'defi_interest', 'defi_endgame']\n",
    "\n",
    "\n",
    "open_ended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKIP stemming & tokenzation to see if yields more interpretable results ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is Part 2 of Onboard Survey Exploratory Analysis\n",
    "\n",
    "# For Part 1 see onboard_survey_open_ended.ipynb\n",
    "# For Part 1 https://forum.bankless.community/t/onboard-survey-exploratory-analysis/1048\n",
    "\n",
    "# Part 2 Open-Ended questions to address include:\n",
    "\n",
    "# What has been painful about using DeFi apps or what has or is an obstacle in your way to using a DeFi app? [column: defi_pain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling\n",
    "\n",
    "# Preparing Text Data for NLP\n",
    "# Goal: Turn text data in to matrix (row = document, column = feature)\n",
    "\n",
    "# Steps: \n",
    "\n",
    "# forming a corpus of text\n",
    "# stemming and lemmatization --- SKIP\n",
    "# tokenization               --- SKIP\n",
    "# removing stop-words\n",
    "# finding words co-located together (N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stemming & Lemmatization to defi_pain\n",
    "\n",
    "# take entire column in open_ended df\n",
    "# split sentences (each row) into words\n",
    "# store in empty list\n",
    "\n",
    "defi_pain_list = []\n",
    "\n",
    "# 12 Rows Removed\n",
    "for row in open_ended['defi_pain']:\n",
    "    try:\n",
    "        defi_pain_list.append(row.split())\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "defi_pain_list  # this is a Nested list - list of list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step need to FLATTEN a list of lists into one list/vector of words - \"Bag of Words\"\n",
    "# Bag of word, a list cleaned of punctuation, stemmed, now a vector of individual words\n",
    "\n",
    "defi_pain_list_flat = [item for sublist in defi_pain_list for item in sublist]\n",
    "\n",
    "defi_pain_list_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF: Weighting terms based on frequency\n",
    "\n",
    "# re-weights words to emphasize words that are unique to a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n",
      "lie\n",
      "systemat\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# Example of how a Stemmer works - - Only used for Stopwords in THIS Notebook\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem('lying'))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Function - - Only used for Stopwords in THIS Notebook\n",
    "\n",
    "def tokenize(text):\n",
    "    translator=str.maketrans(string.punctuation, ' '*len(string.punctuation)) # translator replace punct w empty space\n",
    "    return [stemmer.stem(i) for i in text.translate(translator).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Stopwords\n",
    "stop = stopwords.words('english') + ['invent', 'produce', 'method', 'use', 'first', 'second']\n",
    "full_stopwords = [tokenize(s)[0] for s in stop]\n",
    "\n",
    "full_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try tokenize, then sort dictionary\n",
    "# Conclusion: Tokenize, get rid of stop-words, then sort\n",
    "\n",
    "defi_pain_tokenize = []\n",
    "\n",
    "for list in defi_pain_list:\n",
    "    for word in list:\n",
    "        defi_pain_tokenize.append(tokenize(word))  # This ia a \"Bag of Words\" - a list\n",
    "        \n",
    "defi_pain_tokenize\n",
    "\n",
    "# Last step need to FLATTEN a list of lists into one list/vector of words - \"Bag of Words\"\n",
    "# Bag of word, a list cleaned of punctuation, stemmed, now a vector of individual words\n",
    "\n",
    "defi_pain_tokenize_flat = [item for sublist in defi_pain_tokenize for item in sublist]\n",
    "\n",
    "defi_pain_tokenize_flat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_dict = {}\n",
    "\n",
    "# loop through all elements in list and store count\n",
    "for word in defi_pain_tokenize_flat:\n",
    "    if word not in empty_dict:\n",
    "        empty_dict[word] = 1\n",
    "    else:\n",
    "        empty_dict[word] += 1\n",
    "\n",
    "# sorted\n",
    "sort_empty_dict = sorted(empty_dict.items(), key=lambda x:x[1])\n",
    "sort_empty_dict2 = dict(sort_empty_dict)\n",
    "pprint(sort_empty_dict2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 volatil, faith, almost, near, huge, riski, due, uni, sketchi, earli\n",
      "1 rural, fair, v3, simplic, gwei, dead, select, found, howev, featur\n",
      "2 area, ground, leap, prohibit, function, emiss, teach, rate, gas transact, made\n",
      "3 exposur, exact, crazi, incurr, faint, sinc, recent, wrong, lower, lps\n",
      "4 certain, rare, platform, 5, action, error, cefi, initi, move, legitimaci\n"
     ]
    }
   ],
   "source": [
    "tf_defi_pain_vectorizer = CountVectorizer(analyzer= 'word',  # unit of features are single words rather than phrases\n",
    "                               tokenizer=tokenize, # function to create tokens\n",
    "                               ngram_range=(0,2),   # Allow for bigrams\n",
    "                               strip_accents='unicode',\n",
    "                               stop_words=full_stopwords,  # see above Example Stopwords, other examples did NOT hv stop_words\n",
    "                               min_df = 0.0,\n",
    "                               max_df = 1)   # got an error to lower min_df and raise max_df\n",
    "\n",
    "# Creating bag of words \n",
    "tf_defi_pain_bag_of_words = tf_defi_pain_vectorizer.fit_transform(defi_pain_list_flat) # IMPORTANT transform our UN-Tokenized (no stemming) corpus into a bag of words\n",
    "tf_defi_pain_features = tf_defi_pain_vectorizer.get_feature_names()\n",
    "\n",
    "# Use TfidfTransformer (see library import) to re-weight bag of words\n",
    "tf_defi_pain_transformer = TfidfTransformer(norm = None, smooth_idf = True, sublinear_tf = True)\n",
    "tf_defi_pain_tfidf = tf_defi_pain_transformer.fit_transform(tf_defi_pain_bag_of_words)\n",
    "\n",
    "# Fitting LDA Model\n",
    "tf_defi_pain_lda = LatentDirichletAllocation(n_components = 5, learning_method='online')  # NOTE: n_components = 5\n",
    "tf_defi_pain_doctopic = tf_defi_pain_lda.fit_transform(tf_defi_pain_tfidf)\n",
    "\n",
    "# Displaying the top keywords in each topic\n",
    "tf_defi_pain_keywords_list = []\n",
    "\n",
    "\n",
    "for i, topic in enumerate(tf_defi_pain_lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:10]     # NOTE: 10 instead of 5\n",
    "    tf_defi_pain_keywords = ', '.join(tf_defi_pain_features[i] for i in word_idx)\n",
    "    tf_defi_pain_keywords_list.append(tf_defi_pain_keywords)\n",
    "    print(i, tf_defi_pain_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gwei, select, simplic, action, riski, howev, huge, 5, earli, trial',\n",
       " 'certain, platform, rate, dead, v3, found, featur, sketchi, lps, ledger',\n",
       " 'faith, rare, emiss, leap, ground, teach, near, thank, lower, crazi',\n",
       " 'exposur, area, volatil, function, fair, rural, exact, uni, simpl, due',\n",
       " 'almost, prohibit, gas transact, stablecoin, sinc, faint, ok, cefi, incurr, known']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_defi_pain_keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       ...,\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.02200125, 0.02200123, 0.02200123, 0.91199506, 0.02200124]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_defi_pain_doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gwei, select, simplic, action, riski, howev, huge, 5, earli, trial</th>\n",
       "      <th>certain, platform, rate, dead, v3, found, featur, sketchi, lps, ledger</th>\n",
       "      <th>faith, rare, emiss, leap, ground, teach, near, thank, lower, crazi</th>\n",
       "      <th>exposur, area, volatil, function, fair, rural, exact, uni, simpl, due</th>\n",
       "      <th>almost, prohibit, gas transact, stablecoin, sinc, faint, ok, cefi, incurr, known</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022002</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.911994</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.911994</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gwei, select, simplic, action, riski, howev, huge, 5, earli, trial  \\\n",
       "0                                           0.200000                    \n",
       "1                                           0.200000                    \n",
       "2                                           0.200000                    \n",
       "3                                           0.022002                    \n",
       "4                                           0.911994                    \n",
       "\n",
       "   certain, platform, rate, dead, v3, found, featur, sketchi, lps, ledger  \\\n",
       "0                                           0.200000                        \n",
       "1                                           0.200000                        \n",
       "2                                           0.200000                        \n",
       "3                                           0.022001                        \n",
       "4                                           0.022001                        \n",
       "\n",
       "   faith, rare, emiss, leap, ground, teach, near, thank, lower, crazi  \\\n",
       "0                                           0.200000                    \n",
       "1                                           0.200000                    \n",
       "2                                           0.200000                    \n",
       "3                                           0.022001                    \n",
       "4                                           0.022001                    \n",
       "\n",
       "   exposur, area, volatil, function, fair, rural, exact, uni, simpl, due  \\\n",
       "0                                           0.200000                       \n",
       "1                                           0.200000                       \n",
       "2                                           0.200000                       \n",
       "3                                           0.911994                       \n",
       "4                                           0.022001                       \n",
       "\n",
       "   almost, prohibit, gas transact, stablecoin, sinc, faint, ok, cefi, incurr, known  \n",
       "0                                           0.200000                                 \n",
       "1                                           0.200000                                 \n",
       "2                                           0.200000                                 \n",
       "3                                           0.022001                                 \n",
       "4                                           0.022001                                 "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defi_pain_df = pd.DataFrame(tf_defi_pain_doctopic, columns = tf_defi_pain_keywords_list)\n",
    "\n",
    "defi_pain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conclusion: Tokenize, get rid of stop-words, then sort & count values ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
