{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw/Onboard_Survey.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip first seven columns df.iloc[:, 0:7].head()\n",
    "\n",
    "# selecting only open-ended responses \n",
    "df.iloc[:, 6:14].head()\n",
    "\n",
    "open_ended = df.iloc[:, 6:14]\n",
    "\n",
    "open_ended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>walletwhat_walletwhy</th>\n",
       "      <th>wallet_pain</th>\n",
       "      <th>defi_when</th>\n",
       "      <th>defiwhat_defiwhy</th>\n",
       "      <th>defi_pain</th>\n",
       "      <th>defi_outcome</th>\n",
       "      <th>defi_interest</th>\n",
       "      <th>defi_endgame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trezor - needed cold storage.</td>\n",
       "      <td>keeping up with all the security parameters</td>\n",
       "      <td>Within the last year</td>\n",
       "      <td>uniswap - seems to have a stellar reputation.</td>\n",
       "      <td>Learning how to navigate web3 websites.</td>\n",
       "      <td>Discovered new financial products and revenue ...</td>\n",
       "      <td>Alchemix</td>\n",
       "      <td>Passive income through DeFi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trustwallet, was not knowing much,</td>\n",
       "      <td>still not coming to terms, which wallet to use...</td>\n",
       "      <td>I have never used DeFi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coinbase, ease of transactions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Within the last year</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAVE</td>\n",
       "      <td>Move my traditional investments over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trezor, it just works and its secure</td>\n",
       "      <td>setting up is painful, and dealing with the se...</td>\n",
       "      <td>Within the last year</td>\n",
       "      <td>Uniswap, i had to trade between assets</td>\n",
       "      <td>Gas fees are fluctuating each second</td>\n",
       "      <td>lost money from weird protocols</td>\n",
       "      <td>Options</td>\n",
       "      <td>Become a DeFi native and have more DeFi assets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coinbase bc it was a whileee ago</td>\n",
       "      <td>Feees, centralization etc</td>\n",
       "      <td>3-5 years ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   walletwhat_walletwhy  \\\n",
       "0         Trezor - needed cold storage.   \n",
       "1   Trustwallet, was not knowing much,    \n",
       "2        Coinbase, ease of transactions   \n",
       "3  trezor, it just works and its secure   \n",
       "4      Coinbase bc it was a whileee ago   \n",
       "\n",
       "                                         wallet_pain               defi_when  \\\n",
       "0        keeping up with all the security parameters    Within the last year   \n",
       "1  still not coming to terms, which wallet to use...  I have never used DeFi   \n",
       "2                                                NaN    Within the last year   \n",
       "3  setting up is painful, and dealing with the se...    Within the last year   \n",
       "4                          Feees, centralization etc           3-5 years ago   \n",
       "\n",
       "                                defiwhat_defiwhy  \\\n",
       "0  uniswap - seems to have a stellar reputation.   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3         Uniswap, i had to trade between assets   \n",
       "4                                            NaN   \n",
       "\n",
       "                                 defi_pain  \\\n",
       "0  Learning how to navigate web3 websites.   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3     Gas fees are fluctuating each second   \n",
       "4                                      NaN   \n",
       "\n",
       "                                        defi_outcome defi_interest  \\\n",
       "0  Discovered new financial products and revenue ...      Alchemix   \n",
       "1                                                NaN           NaN   \n",
       "2                                                NaN          AAVE   \n",
       "3                    lost money from weird protocols       Options   \n",
       "4                                                NaN           NaN   \n",
       "\n",
       "                                        defi_endgame  \n",
       "0                        Passive income through DeFi  \n",
       "1                                                NaN  \n",
       "2               Move my traditional investments over  \n",
       "3  Become a DeFi native and have more DeFi assets...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns to better manage columns\n",
    "open_ended.columns = ['walletwhat_walletwhy', 'wallet_pain', 'defi_when', 'defiwhat_defiwhy', 'defi_pain', 'defi_outcome', 'defi_interest', 'defi_endgame']\n",
    "\n",
    "\n",
    "open_ended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Part 2 of Onboard Survey Exploratory Analysis\n",
    "\n",
    "# For Part 1 see onboard_survey_open_ended.ipynb\n",
    "# For Part 1 https://forum.bankless.community/t/onboard-survey-exploratory-analysis/1048\n",
    "\n",
    "# Part 2 Open-Ended questions to address include:\n",
    "\n",
    "# What has been painful about using DeFi apps or what has or is an obstacle in your way to using a DeFi app? [column: defi_pain]\n",
    "# Tell us about one positive or unexpected outcome you had from using a DeFi app? [column: defi_outcome]\n",
    "# What DeFi app are you most interested in using? [column: defi_interest]\n",
    "# What is your DeFi endgame? [column: defi_endgame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual columns of interest\n",
    "open_ended.iloc[:, 4:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on 1 column first, defi_pain\n",
    "# What has been painful about using DeFi apps or what has or is an obstacle in your way to using a DeFi app? [column: defi_pain]\n",
    "\n",
    "open_ended['defi_pain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling\n",
    "\n",
    "# Preparing Text Data for NLP\n",
    "# Goal: Turn text data in to matrix (row = document, column = feature)\n",
    "\n",
    "# Steps: \n",
    "\n",
    "# forming a corpus of text\n",
    "# stemming and lemmatization\n",
    "# tokenization\n",
    "# removing stop-words\n",
    "# finding words co-located together (N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n",
      "lie\n",
      "systemat\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# Example of how a Stemmer works\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem('lying'))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stemming & Lemmatization to defi_pain\n",
    "\n",
    "# take entire column in open_ended df\n",
    "# split sentences (each row) into words\n",
    "# store in empty list\n",
    "\n",
    "defi_pain_list = []\n",
    "\n",
    "# 12 Rows Removed\n",
    "for row in open_ended['defi_pain']:\n",
    "    try:\n",
    "        defi_pain_list.append(row.split())\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "defi_pain_list  # this is a Nested list - list of list; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through defi_pain_list[0] and apply stemming\n",
    "\n",
    "for word in defi_pain_list[0]:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through defi_pain_list (list of list) and apply stemming\n",
    "\n",
    "for list in defi_pain_list:\n",
    "    for word in list:\n",
    "        print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'websites '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Removing Punctuation\n",
    "\n",
    "# Before\n",
    "defi_pain_list[0]\n",
    "\n",
    "# Create translator\n",
    "translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "\n",
    "# After removes \".\" period in \"websites.\"\n",
    "defi_pain_list[0][5].translate(translator)\n",
    "\n",
    "# NOTE: This only works on indiviual strings/words, NOT on lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing\n",
    "\n",
    "# Create a function to take a string, split into individual words, \n",
    "# Remove punctuation, stemming and tokenizing all in ONE function\n",
    "\n",
    "# overlaps slightly with above\n",
    "\n",
    "# 12 rows got removed with defi_pain_list\n",
    "\n",
    "defi_pain_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Function\n",
    "\n",
    "def tokenize(text):\n",
    "    translator=str.maketrans(string.punctuation, ' '*len(string.punctuation)) # translator replace punct w empty space\n",
    "    return [stemmer.stem(i) for i in text.translate(translator).split()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through list of list (defi_pain_list) \n",
    "# Apply tokenize() function\n",
    "# save output to new list\n",
    "# output needs to be a vector of individual words\n",
    "\n",
    "# NOTE: Because tokenize() function returns a list, each word will be put into it's own list\n",
    "\n",
    "defi_pain_tokenize = []\n",
    "\n",
    "for list in defi_pain_list:\n",
    "    for word in list:\n",
    "        defi_pain_tokenize.append(tokenize(word))  # This ia a \"Bag of Words\" - a list\n",
    "        \n",
    "defi_pain_tokenize\n",
    "\n",
    "# Last step need to FLATTEN a list of lists into one list/vector of words - \"Bag of Words\"\n",
    "# Bag of word, a list cleaned of punctuation, stemmed, now a vector of individual words\n",
    "\n",
    "defi_pain_tokenize_flat = [item for sublist in defi_pain_tokenize for item in sublist]\n",
    "\n",
    "defi_pain_tokenize_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer, a library imported from sklearn, that will tokenize, \n",
    "# but also count duplicates of words and create a matrix that contains the frequency of each word\n",
    "# This is large matrix, so the output is a sparse matrix\n",
    "\n",
    "# Process: (similar to fitting models in sklearn), we create the vectorizer object\n",
    "# then fit each word to give an overall corpus bag of words and list of features (unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\",\n",
    "                            tokenizer=tokenize,\n",
    "                            ngram_range=(0,1),\n",
    "                            strip_accents='unicode',\n",
    "                            min_df = 0.0,\n",
    "                            max_df = 1)        # got an error to lower min_df and raise max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defi_pain_bag_of_words = vectorizer.fit_transform(defi_pain_tokenize_flat) # transform our corpus into a bag of words\n",
    "defi_pain_features = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "defi_pain_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(defi_pain_bag_of_words)\n",
    "#print(defi_pain_features)\n",
    "\n",
    "#defi_pain_features[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "lda = LatentDirichletAllocation(learning_method='online') \n",
    "\n",
    "doctopic = lda.fit_transform( defi_pain_bag_of_words )\n",
    "\n",
    "doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This might not look helpful at first\n",
    "\n",
    "defi_pain_keywords_list = []\n",
    "\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:5]      # NOTE: 5\n",
    "    defi_pain_keywords = ', '.join(defi_pain_features[i] for i in word_idx)\n",
    "    defi_pain_keywords_list.append(defi_pain_keywords)\n",
    "    print(i, defi_pain_keywords)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: The above does not appear to contain stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## N-Grams: Adding context by creating N-Grams\n",
    "\n",
    "# instead of treating each word as an individual unit\n",
    "# treat each group of 2 words or 3 words or n-words as a unit\n",
    "# \"Bag of n-grams\", where n is the number of words in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 uni, rural, v3, area, ground, huge, trial, wrong, action, known\n",
      "1 won, faith, leap, found, howev, certain, error, gwei, move, incurr\n",
      "2 rare, teach, riski, simplic, exact, recent, near, stablecoin, select, initi\n",
      "3 function, almost, simpl, featur, fair, emiss, dead, exposur, prohibit, rate\n",
      "4 your, volatil, platform, faint, thank, gaug, lps, heard, justifi, combo\n"
     ]
    }
   ],
   "source": [
    "bi_vectorizer = CountVectorizer(analyzer= \"word\",\n",
    "                                tokenizer=tokenize,\n",
    "                                ngram_range=(0,2),          # Allow for bigrams\n",
    "                                strip_accents='unicode',\n",
    "                                min_df = 0.0,\n",
    "                                max_df = 1)      # got an error to lower min_df and raise max_df\n",
    "\n",
    "# Creating bag of words\n",
    "bi_defi_pain_bag_of_words = bi_vectorizer.fit_transform(defi_pain_tokenize_flat) # transform our corpus into a bag of words\n",
    "bi_defi_pain_features = bi_vectorizer.get_feature_names()\n",
    "\n",
    "# Fitting LDA Model\n",
    "bi_lda = LatentDirichletAllocation(n_components = 5, learning_method='online')   # NOTE: n_components = 5\n",
    "bi_doctopic = bi_lda.fit_transform(bi_defi_pain_bag_of_words)\n",
    "\n",
    "# Display the top keywords in each topic\n",
    "bi_defi_pain_keywords_list = []\n",
    "\n",
    "for i, topic in enumerate(bi_lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:10]     # NOTE: 10 instead of 5\n",
    "    bi_defi_pain_keywords = ', '.join(bi_defi_pain_features[i] for i in word_idx)\n",
    "    bi_defi_pain_keywords_list.append(bi_defi_pain_keywords)\n",
    "    print(i, bi_defi_pain_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF: Weighting terms based on frequency\n",
    "\n",
    "# re-weights words to emphasize words that are unique to a document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'our',\n",
       " 'ourselv',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'your',\n",
       " 'your',\n",
       " 'yourself',\n",
       " 'yourselv',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'she',\n",
       " 'her',\n",
       " 'her',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'it',\n",
       " 'it',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'their',\n",
       " 'themselv',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'be',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'have',\n",
       " 'do',\n",
       " 'doe',\n",
       " 'did',\n",
       " 'do',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'becaus',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'dure',\n",
       " 'befor',\n",
       " 'after',\n",
       " 'abov',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'onc',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whi',\n",
       " 'how',\n",
       " 'all',\n",
       " 'ani',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'onli',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'veri',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'don',\n",
       " 'should',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'wouldn',\n",
       " 'invent',\n",
       " 'produc',\n",
       " 'method',\n",
       " 'use',\n",
       " 'first',\n",
       " 'second']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Stopwords\n",
    "stop = stopwords.words('english') + ['invent', 'produce', 'method', 'use', 'first', 'second']\n",
    "full_stopwords = [tokenize(s)[0] for s in stop]\n",
    "\n",
    "full_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 faith, area, dead, incurr, cefi, error, simplic, stablecoin, trial, action\n",
      "1 rural, emiss, howev, v3, almost, huge, due, exact, sinc, project\n",
      "2 volatil, rare, featur, ground, initi, faint, goal, two, worri, ledger\n",
      "3 teach, prohibit, found, simpl, platform, uni, certain, fair, exposur, recent\n",
      "4 function, rate, leap, gwei, riski, 5, lower, move, known, wrong\n"
     ]
    }
   ],
   "source": [
    "tf_defi_pain_vectorizer = CountVectorizer(analyzer= 'word',  # unit of features are single words rather than phrases\n",
    "                               tokenizer=tokenize, # function to create tokens\n",
    "                               ngram_range=(0,2),   # Allow for bigrams\n",
    "                               strip_accents='unicode',\n",
    "                               stop_words=full_stopwords,  # see above Example Stopwords, other examples did NOT hv stop_words\n",
    "                               min_df = 0.0,\n",
    "                               max_df = 1)   # got an error to lower min_df and raise max_df\n",
    "\n",
    "# Creating bag of words \n",
    "tf_defi_pain_bag_of_words = tf_defi_pain_vectorizer.fit_transform(defi_pain_tokenize_flat) # transform our corpus into a bag of words\n",
    "tf_defi_pain_features = tf_defi_pain_vectorizer.get_feature_names()\n",
    "\n",
    "# Use TfidfTransformer (see library import) to re-weight bag of words\n",
    "tf_defi_pain_transformer = TfidfTransformer(norm = None, smooth_idf = True, sublinear_tf = True)\n",
    "tf_defi_pain_tfidf = tf_defi_pain_transformer.fit_transform(tf_defi_pain_bag_of_words)\n",
    "\n",
    "# Fitting LDA Model\n",
    "tf_defi_pain_lda = LatentDirichletAllocation(n_components = 5, learning_method='online')  # NOTE: n_components = 5\n",
    "tf_defi_pain_doctopic = tf_defi_pain_lda.fit_transform(tf_defi_pain_tfidf)\n",
    "\n",
    "# Displaying the top keywords in each topic\n",
    "tf_defi_pain_keywords_list = []\n",
    "\n",
    "\n",
    "for i, topic in enumerate(tf_defi_pain_lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:10]     # NOTE: 10 instead of 5\n",
    "    tf_defi_pain_keywords = ', '.join(tf_defi_pain_features[i] for i in word_idx)\n",
    "    tf_defi_pain_keywords_list.append(tf_defi_pain_keywords)\n",
    "    print(i, tf_defi_pain_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faith, area, dead, incurr, cefi, error, simplic, stablecoin, trial, action',\n",
       " 'rural, emiss, howev, v3, almost, huge, due, exact, sinc, project',\n",
       " 'volatil, rare, featur, ground, initi, faint, goal, two, worri, ledger',\n",
       " 'teach, prohibit, found, simpl, platform, uni, certain, fair, exposur, recent',\n",
       " 'function, rate, leap, gwei, riski, 5, lower, move, known, wrong']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_defi_pain_keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       ...,\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.02195422, 0.02195423, 0.9121831 , 0.02195422, 0.02195422]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_defi_pain_doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faith, area, dead, incurr, cefi, error, simplic, stablecoin, trial, action</th>\n",
       "      <th>rural, emiss, howev, v3, almost, huge, due, exact, sinc, project</th>\n",
       "      <th>volatil, rare, featur, ground, initi, faint, goal, two, worri, ledger</th>\n",
       "      <th>teach, prohibit, found, simpl, platform, uni, certain, fair, exposur, recent</th>\n",
       "      <th>function, rate, leap, gwei, riski, 5, lower, move, known, wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.912181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.912181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   faith, area, dead, incurr, cefi, error, simplic, stablecoin, trial, action  \\\n",
       "0                                           0.200000                            \n",
       "1                                           0.200000                            \n",
       "2                                           0.200000                            \n",
       "3                                           0.021955                            \n",
       "4                                           0.021955                            \n",
       "\n",
       "   rural, emiss, howev, v3, almost, huge, due, exact, sinc, project  \\\n",
       "0                                           0.200000                  \n",
       "1                                           0.200000                  \n",
       "2                                           0.200000                  \n",
       "3                                           0.021955                  \n",
       "4                                           0.021955                  \n",
       "\n",
       "   volatil, rare, featur, ground, initi, faint, goal, two, worri, ledger  \\\n",
       "0                                           0.200000                       \n",
       "1                                           0.200000                       \n",
       "2                                           0.200000                       \n",
       "3                                           0.021955                       \n",
       "4                                           0.021955                       \n",
       "\n",
       "   teach, prohibit, found, simpl, platform, uni, certain, fair, exposur, recent  \\\n",
       "0                                           0.200000                              \n",
       "1                                           0.200000                              \n",
       "2                                           0.200000                              \n",
       "3                                           0.021955                              \n",
       "4                                           0.021955                              \n",
       "\n",
       "   function, rate, leap, gwei, riski, 5, lower, move, known, wrong  \n",
       "0                                           0.200000                \n",
       "1                                           0.200000                \n",
       "2                                           0.200000                \n",
       "3                                           0.912181                \n",
       "4                                           0.912181                "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defi_pain_df = pd.DataFrame(tf_defi_pain_doctopic, columns = tf_defi_pain_keywords_list)\n",
    "\n",
    "defi_pain_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
