{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw/Onboard_Survey.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skip first seven columns df.iloc[:, 0:7].head()\n",
    "\n",
    "# selecting only open-ended responses \n",
    "df.iloc[:, 6:14].head()\n",
    "\n",
    "open_ended = df.iloc[:, 6:14]\n",
    "\n",
    "open_ended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>walletwhat_walletwhy</th>\n",
       "      <th>wallet_pain</th>\n",
       "      <th>defi_when</th>\n",
       "      <th>defiwhat_defiwhy</th>\n",
       "      <th>defi_pain</th>\n",
       "      <th>defi_outcome</th>\n",
       "      <th>defi_interest</th>\n",
       "      <th>defi_endgame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trezor - needed cold storage.</td>\n",
       "      <td>keeping up with all the security parameters</td>\n",
       "      <td>Within the last year</td>\n",
       "      <td>uniswap - seems to have a stellar reputation.</td>\n",
       "      <td>Learning how to navigate web3 websites.</td>\n",
       "      <td>Discovered new financial products and revenue ...</td>\n",
       "      <td>Alchemix</td>\n",
       "      <td>Passive income through DeFi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trustwallet, was not knowing much,</td>\n",
       "      <td>still not coming to terms, which wallet to use...</td>\n",
       "      <td>I have never used DeFi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coinbase, ease of transactions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Within the last year</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAVE</td>\n",
       "      <td>Move my traditional investments over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trezor, it just works and its secure</td>\n",
       "      <td>setting up is painful, and dealing with the se...</td>\n",
       "      <td>Within the last year</td>\n",
       "      <td>Uniswap, i had to trade between assets</td>\n",
       "      <td>Gas fees are fluctuating each second</td>\n",
       "      <td>lost money from weird protocols</td>\n",
       "      <td>Options</td>\n",
       "      <td>Become a DeFi native and have more DeFi assets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coinbase bc it was a whileee ago</td>\n",
       "      <td>Feees, centralization etc</td>\n",
       "      <td>3-5 years ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   walletwhat_walletwhy  \\\n",
       "0         Trezor - needed cold storage.   \n",
       "1   Trustwallet, was not knowing much,    \n",
       "2        Coinbase, ease of transactions   \n",
       "3  trezor, it just works and its secure   \n",
       "4      Coinbase bc it was a whileee ago   \n",
       "\n",
       "                                         wallet_pain               defi_when  \\\n",
       "0        keeping up with all the security parameters    Within the last year   \n",
       "1  still not coming to terms, which wallet to use...  I have never used DeFi   \n",
       "2                                                NaN    Within the last year   \n",
       "3  setting up is painful, and dealing with the se...    Within the last year   \n",
       "4                          Feees, centralization etc           3-5 years ago   \n",
       "\n",
       "                                defiwhat_defiwhy  \\\n",
       "0  uniswap - seems to have a stellar reputation.   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3         Uniswap, i had to trade between assets   \n",
       "4                                            NaN   \n",
       "\n",
       "                                 defi_pain  \\\n",
       "0  Learning how to navigate web3 websites.   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3     Gas fees are fluctuating each second   \n",
       "4                                      NaN   \n",
       "\n",
       "                                        defi_outcome defi_interest  \\\n",
       "0  Discovered new financial products and revenue ...      Alchemix   \n",
       "1                                                NaN           NaN   \n",
       "2                                                NaN          AAVE   \n",
       "3                    lost money from weird protocols       Options   \n",
       "4                                                NaN           NaN   \n",
       "\n",
       "                                        defi_endgame  \n",
       "0                        Passive income through DeFi  \n",
       "1                                                NaN  \n",
       "2               Move my traditional investments over  \n",
       "3  Become a DeFi native and have more DeFi assets...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# rename columns to better manage columns\n",
    "open_ended.columns = ['walletwhat_walletwhy', 'wallet_pain', 'defi_when', 'defiwhat_defiwhy', 'defi_pain', 'defi_outcome', 'defi_interest', 'defi_endgame']\n",
    "\n",
    "\n",
    "open_ended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is Part 2 of Onboard Survey Exploratory Analysis\n",
    "\n",
    "# For Part 1 see onboard_survey_open_ended.ipynb\n",
    "# For Part 1 https://forum.bankless.community/t/onboard-survey-exploratory-analysis/1048\n",
    "\n",
    "# Part 2 Open-Ended questions to address include:\n",
    "\n",
    "# What has been painful about using DeFi apps or what has or is an obstacle in your way to using a DeFi app? [column: defi_pain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling\n",
    "\n",
    "# Preparing Text Data for NLP\n",
    "# Goal: Turn text data in to matrix (row = document, column = feature)\n",
    "\n",
    "# Steps: \n",
    "\n",
    "# forming a corpus of text\n",
    "# stemming and lemmatization\n",
    "# tokenization\n",
    "# removing stop-words\n",
    "# finding words co-located together (N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how a Stemmer works\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem('lying'))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stemming & Lemmatization to defi_pain\n",
    "\n",
    "# take entire column in open_ended df\n",
    "# split sentences (each row) into words\n",
    "# store in empty list\n",
    "\n",
    "defi_pain_list = []\n",
    "\n",
    "# 12 Rows Removed\n",
    "for row in open_ended['defi_pain']:\n",
    "    try:\n",
    "        defi_pain_list.append(row.split())\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "defi_pain_list  # this is a Nested list - list of list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Function\n",
    "\n",
    "def tokenize(text):\n",
    "    translator=str.maketrans(string.punctuation, ' '*len(string.punctuation)) # translator replace punct w empty space\n",
    "    return [stemmer.stem(i) for i in text.translate(translator).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through list of list (defi_pain_list) \n",
    "# Apply tokenize() function\n",
    "# save output to new list\n",
    "# output needs to be a vector of individual words\n",
    "\n",
    "# NOTE: Because tokenize() function returns a list, each word will be put into it's own list\n",
    "\n",
    "defi_pain_tokenize = []\n",
    "\n",
    "for list in defi_pain_list:\n",
    "    for word in list:\n",
    "        defi_pain_tokenize.append(tokenize(word))  # This ia a \"Bag of Words\" - a list\n",
    "        \n",
    "defi_pain_tokenize\n",
    "\n",
    "# Last step need to FLATTEN a list of lists into one list/vector of words - \"Bag of Words\"\n",
    "# Bag of word, a list cleaned of punctuation, stemmed, now a vector of individual words\n",
    "\n",
    "defi_pain_tokenize_flat = [item for sublist in defi_pain_tokenize for item in sublist]\n",
    "\n",
    "defi_pain_tokenize_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TF-IDF: Weighting terms based on frequency\n",
    "\n",
    "# re-weights words to emphasize words that are unique to a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Stopwords\n",
    "stop = stopwords.words('english') + ['invent', 'produce', 'method', 'use', 'first', 'second']\n",
    "full_stopwords = [tokenize(s)[0] for s in stop]\n",
    "\n",
    "full_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 uni, v3, area, dead, rural, certain, fair, leap, sinc, scare\n",
      "1 teach, howev, emiss, almost, platform, prohibit, rate, simpl, known, 5\n",
      "2 near, move, incurr, cefi, initi, recent, error, action, 2, legitimaci\n",
      "3 found, simplic, exact, riski, due, select, trial, stablecoin, lower, lps\n",
      "4 faith, volatil, function, rare, ground, exposur, huge, featur, gwei, thank\n"
     ]
    }
   ],
   "source": [
    "tf_defi_pain_vectorizer = CountVectorizer(analyzer= 'word',  # unit of features are single words rather than phrases\n",
    "                               tokenizer=tokenize, # function to create tokens\n",
    "                               ngram_range=(0,2),   # Allow for bigrams\n",
    "                               strip_accents='unicode',\n",
    "                               stop_words=full_stopwords,  # see above Example Stopwords, other examples did NOT hv stop_words\n",
    "                               min_df = 0.0,\n",
    "                               max_df = 1)   # got an error to lower min_df and raise max_df\n",
    "\n",
    "# Creating bag of words \n",
    "tf_defi_pain_bag_of_words = tf_defi_pain_vectorizer.fit_transform(defi_pain_tokenize_flat) # transform our corpus into a bag of words\n",
    "tf_defi_pain_features = tf_defi_pain_vectorizer.get_feature_names()\n",
    "\n",
    "# Use TfidfTransformer (see library import) to re-weight bag of words\n",
    "tf_defi_pain_transformer = TfidfTransformer(norm = None, smooth_idf = True, sublinear_tf = True)\n",
    "tf_defi_pain_tfidf = tf_defi_pain_transformer.fit_transform(tf_defi_pain_bag_of_words)\n",
    "\n",
    "# Fitting LDA Model\n",
    "tf_defi_pain_lda = LatentDirichletAllocation(n_components = 5, learning_method='online')  # NOTE: n_components = 5\n",
    "tf_defi_pain_doctopic = tf_defi_pain_lda.fit_transform(tf_defi_pain_tfidf)\n",
    "\n",
    "# Displaying the top keywords in each topic\n",
    "tf_defi_pain_keywords_list = []\n",
    "\n",
    "\n",
    "for i, topic in enumerate(tf_defi_pain_lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:10]     # NOTE: 10 instead of 5\n",
    "    tf_defi_pain_keywords = ', '.join(tf_defi_pain_features[i] for i in word_idx)\n",
    "    tf_defi_pain_keywords_list.append(tf_defi_pain_keywords)\n",
    "    print(i, tf_defi_pain_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uni, v3, area, dead, rural, certain, fair, leap, sinc, scare',\n",
       " 'teach, howev, emiss, almost, platform, prohibit, rate, simpl, known, 5',\n",
       " 'near, move, incurr, cefi, initi, recent, error, action, 2, legitimaci',\n",
       " 'found, simplic, exact, riski, due, select, trial, stablecoin, lower, lps',\n",
       " 'faith, volatil, function, rare, ground, exposur, huge, featur, gwei, thank']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_defi_pain_keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       ...,\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.02195424, 0.02195424, 0.02195424, 0.02195424, 0.91218305]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_defi_pain_doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uni, v3, area, dead, rural, certain, fair, leap, sinc, scare</th>\n",
       "      <th>teach, howev, emiss, almost, platform, prohibit, rate, simpl, known, 5</th>\n",
       "      <th>near, move, incurr, cefi, initi, recent, error, action, 2, legitimaci</th>\n",
       "      <th>found, simplic, exact, riski, due, select, trial, stablecoin, lower, lps</th>\n",
       "      <th>faith, volatil, function, rare, ground, exposur, huge, featur, gwei, thank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.912181</td>\n",
       "      <td>0.021955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.912180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uni, v3, area, dead, rural, certain, fair, leap, sinc, scare  \\\n",
       "0                                           0.200000              \n",
       "1                                           0.200000              \n",
       "2                                           0.200000              \n",
       "3                                           0.021955              \n",
       "4                                           0.021955              \n",
       "\n",
       "   teach, howev, emiss, almost, platform, prohibit, rate, simpl, known, 5  \\\n",
       "0                                           0.200000                        \n",
       "1                                           0.200000                        \n",
       "2                                           0.200000                        \n",
       "3                                           0.021955                        \n",
       "4                                           0.021955                        \n",
       "\n",
       "   near, move, incurr, cefi, initi, recent, error, action, 2, legitimaci  \\\n",
       "0                                           0.200000                       \n",
       "1                                           0.200000                       \n",
       "2                                           0.200000                       \n",
       "3                                           0.021955                       \n",
       "4                                           0.021955                       \n",
       "\n",
       "   found, simplic, exact, riski, due, select, trial, stablecoin, lower, lps  \\\n",
       "0                                           0.200000                          \n",
       "1                                           0.200000                          \n",
       "2                                           0.200000                          \n",
       "3                                           0.912181                          \n",
       "4                                           0.021955                          \n",
       "\n",
       "   faith, volatil, function, rare, ground, exposur, huge, featur, gwei, thank  \n",
       "0                                           0.200000                           \n",
       "1                                           0.200000                           \n",
       "2                                           0.200000                           \n",
       "3                                           0.021955                           \n",
       "4                                           0.912180                           "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defi_pain_df = pd.DataFrame(tf_defi_pain_doctopic, columns = tf_defi_pain_keywords_list)\n",
    "\n",
    "defi_pain_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
